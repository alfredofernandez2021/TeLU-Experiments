# -*- coding: utf-8 -*-
"""Elman_TeLU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b6UY7UlQKMR1x5eYCcHcPvru4GJYCL1k

#Language Model Using a Custom Build LSTM
This notebook contains the code to implement a language model using a custom LSTM built within the PyTorch framework.

This model was implemented as part of a project in the course 02456 Deep Learning @ DTU - Technical University of Denmark
+ This code was originally forked from the [PyTorch word level language modeling example](https://github.com/pytorch/examples/tree/master/word_language_model).
+ The code in this notebook is available on [google colab](https://colab.research.google.com/drive/1luim4qegwBeKVAAzW-XarPSCOhogiBQf) and on [github](https://github.com/mikkelbrusen/custom-pytorch-lstm-lm).

The model comes with instructions to train a word level language models over the Penn Treebank (PTB).

The project was carried out by [Gustav Madslund](https://github.com/gustavmadslund) and [Mikkel Møller Brusen](https://github.com/mikkelbrusen).

# Setup
This section contains all the necessary setup as hyperparameters, data processing and utility functions

## Google Colab Setup
Since we are running on Google Colab, we will need to install PyTorch as they only support TensorFlow by default, because, well, they are Google and not Facebook.
"""

# # http://pytorch.org/
# from os.path import exists
# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
# cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
# accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'

# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision
# import torch

"""We will need some data to train on, and a place to save our model.
We connect to google drive and position our data in the following path: *MyDrive/NLP/data/penn/* which needs to be put in
"""

#from google.colab import drive
#drive.mount('/content/drive')

"""## Imports and params




"""

import argparse
import time
import math
import os
import torch
import torch.nn as nn
from torch.autograd import Variable
from collections import Counter
import copy

args_cuda = torch.cuda.is_available()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyper-parameters
args_train_batch_size = 20 # batch size
args_bptt = 25 #25
args_embed_size = 450 #450
args_hidden_size = 650 #650
args_num_layers = 2 # Number of LSTM layers
args_num_epochs = 40
args_learning_rate = 0.2
args_dropout = 0.2 #0.0
args_clip = 1.0 #0.25
args_log_interval = 100
args_seed = 3 # We seed the RNG's for reproducability

# if you dont already have the penn treebank data, grab it from our github repo
# here: https://github.com/mikkelbrusen/custom-pytorch-lstm-lm
args_data = "PTB/"

# The file in which we want to save our trained model.
#args_save = "/content/gdrive/My Drive/NLP/save/Custom_LSTM_Model.pt"


torch.manual_seed(args_seed)
if args_cuda:
  torch.cuda.manual_seed(args_seed)

"""## The data loader
Dictionary and corpus to process the dataset
"""

class Dictionary(object):
    def __init__(self):
        self.word2idx = {}
        self.idx2word = []
        self.counter = Counter()
        self.total = 0

    def add_word(self, word):
        if word not in self.word2idx:
            self.idx2word.append(word)
            self.word2idx[word] = len(self.idx2word) - 1
        token_id = self.word2idx[word]
        self.counter[token_id] += 1
        self.total += 1
        return self.word2idx[word]

    def __len__(self):
        return len(self.idx2word)


class Corpus(object):
    def __init__(self, path):
        self.dictionary = Dictionary()
        self.train = self.tokenize(os.path.join(path, 'train.txt'))
        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))
        self.test = self.tokenize(os.path.join(path, 'test.txt'))

    def tokenize(self, path):
        """Tokenizes a text file."""
        print(path)
        assert os.path.exists(path)
        # Add words to the dictionary
        with open(path, 'r') as f:
            tokens = 0
            for line in f:
                words = line.split() + ['<eos>']
                tokens += len(words)
                for word in words:
                    self.dictionary.add_word(word)

        # Tokenize file content
        with open(path, 'r') as f:
            ids = torch.LongTensor(tokens)
            token = 0
            for line in f:
                words = line.split() + ['<eos>']
                for word in words:
                    ids[token] = self.dictionary.word2idx[word]
                    token += 1

        return ids

"""## Utils
Utility functions which will be used while training, validating and testing
"""

def repackage_hidden(h):
    """Wraps hidden states in new Tensors, to detach them from their history."""
    if isinstance(h, torch.Tensor):
        return h.detach()
    else:
        return tuple(repackage_hidden(v) for v in h)


# Starting from sequential data, batchify arranges the dataset into columns.
# For instance, with the alphabet as the sequence and batch size 4, we'd get
# ┌ a g m s ┐
# │ b h n t │
# │ c i o u │
# │ d j p v │
# │ e k q w │
# └ f l r x ┘.
# These columns are treated as independent by the model, which means that the
# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient
# batch processing.

def batchify(data, bsz):
    # Work out how cleanly we can divide the dataset into bsz parts.
    nbatch = data.size(0) // bsz
    # Trim off any extra elements that wouldn't cleanly fit (remainders).
    data = data.narrow(0, 0, nbatch * bsz)
    # Evenly divide the data across the bsz batches.
    data = data.view(bsz, -1).t().contiguous()
    return data.to(device)


# get_batch subdivides the source data into chunks of length args.bptt.
# If source is equal to the example output of the batchify function, with
# a bptt-limit of 2, we'd get the following two Variables for i = 0:
# ┌ a g m s ┐ ┌ b h n t ┐
# └ b h n t ┘ └ c i o u ┘
# Note that despite the name of the function, the subdivison of data is not
# done along the batch dimension (i.e. dimension 1), since that was handled
# by the batchify function. The chunks are along dimension 0, corresponding
# to the seq_len dimension in the LSTM.

def get_batch(source, i):
    #seq_len = min(args_bptt, len(source) - 1 - i)
    data = source[i-args_bptt:i]
    target = source[i+1-args_bptt:i+1].view(-1)
    return data, target

"""## Process data
Load the dataset and make train, validaiton and test sets
"""

corpus = Corpus(args_data)

eval_batch_size = 10
test_batch_size = 10
train_data = batchify(corpus.train, args_train_batch_size)
val_data = batchify(corpus.valid, eval_batch_size)
test_data = batchify(corpus.test, test_batch_size)

"""# Custom RNN
We chose to implement our LSTM modules as single layer modules, meaning that the multiple layers will be created within our model rather than within the LSTM module.

### Dimensions
An analysis of the dimensions can be found in the following figures
"""

def telu(input):
    return input * torch.tanh(torch.exp(input))

#RNN Module
class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, bias=False):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size

    self.i_h = nn.Linear(input_size, hidden_size, bias=bias)
    self.h_h = nn.Linear(hidden_size, hidden_size, bias=True)


  def forward(self, input, hidden):

    h = hidden

    def recurrence(inp, hidden):
      """Recurrence helper."""
      h = hidden

      # f_g = torch.sigmoid(self.weight_fx(inp) + self.weight_fh(h))
      # i_g = torch.sigmoid(self.weight_ix(inp) + self.weight_ih(h))
      # o_g = torch.sigmoid(self.weight_ox(inp) + self.weight_oh(h))
      # c_tilda = torch.tanh(self.weight_cx(inp) + self.weight_ch(h))
      # c_t = f_g * c + i_g * c_tilda
      # h_t = o_g * torch.tanh(c_t)

      x = self.i_h(inp)
      #print(hidden.size())
      hidden_state = self.h_h(hidden)
      #print("post hidden")
      hidden_state = telu(x + hidden_state)
      #print("post hidden")
      out = hidden_state

      return out
      #--------------

    output = []
    for inp in input:
      h = recurrence(inp, h)
      output.append(h)
    #print("end unit RNN")

    # torch.cat(output, 0).size()=torch.Size([700, 650]) view(input.size(0)=35, *output[0].size()=20 650)
    output = torch.cat(output, 0).view(input.size(0), *output[0].size())
    #print(input.size(0), *output[0].size())
    return output, h

"""# Language Model using our Custom RNN
First we define our model
"""

class RNNModel(nn.Module):
    def __init__(self, num_tokens, embed_size, hidden_size, output_size, dropout=0.5, n_layers=1):
        super(RNNModel, self).__init__()
        self.drop = nn.Dropout(dropout)
        self.encoder = nn.Embedding(num_tokens, embed_size)

        # We add each LSTM layer to the module list such that pytorch is aware
        # of their parameters for when we perform gradient decent
        self.layers = nn.ModuleList()
        for l in range(n_layers):
          layer_input_size = embed_size if l == 0 else hidden_size
          self.layers.append(RNN(layer_input_size, hidden_size))

        self.decoder = nn.Linear(hidden_size, output_size)

        self.hidden_size = hidden_size
        self.n_layers = n_layers

        self.init_weights()


    def init_weights(self):
        initrange = 0.1
        self.encoder.weight.data.uniform_(-initrange, initrange)
        self.decoder.bias.data.zero_()
        self.decoder.weight.data.uniform_(-initrange, initrange)

    def forward(self, inp, hidden):
        emb = self.drop(self.encoder(inp))

        output= emb
        h_0 = hidden
        h = []

        # Iterate over each RNN layer, and pass the output from one layer on to the next
        for i, layer in enumerate(self.layers):
            output, h_i = layer(output, h_0[i])
            output = self.drop(output)

            h += [h_i]

        h = torch.stack(h)

        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))
        decoded = decoded.view(output.size(0), output.size(1), decoded.size(1))

        return decoded, h

    def init_hidden(self,bsz):
        h_0 = Variable(torch.zeros(self.n_layers, bsz, self.hidden_size)).cuda()
        #c_0 = Variable(torch.zeros(self.n_layers, bsz, self.hidden_size)).cuda()
        return h_0

"""Then we build the model and specify our loss function"""

ntokens = len(corpus.dictionary)

model = RNNModel(ntokens, args_embed_size, args_hidden_size, ntokens, args_dropout, args_num_layers).to(device)
criterion = nn.CrossEntropyLoss()

"""# Train model

First we define our training and evalutation
"""

def evaluate(data_source):
    # Turn on evaluation mode which disables dropout.
    model.eval()
    total_loss = 0.
    total_words = len(data_source) - (len(data_source) % args_bptt)
    ntokens = len(corpus.dictionary)
    hidden = model.init_hidden(test_batch_size)
    with torch.no_grad():
        for i in range(args_bptt, data_source.size(0) - 1, args_bptt):
            data, targets = get_batch(data_source, i)
            output, hidden = model(data, hidden)
            output_flat = output.view(-1, ntokens)
            total_loss += len(data) * criterion(output_flat, targets).item()
            hidden = repackage_hidden(hidden)
    return total_loss / (total_words  - 1)


def train():
    # Turn on training mode which enables dropout.
    model.train()
    total_loss = 0.
    start_time = time.time()
    ntokens = len(corpus.dictionary)
    hidden = model.init_hidden(args_train_batch_size)
    for batch, i in enumerate(range(args_bptt, train_data.size(0) - 1, args_bptt)):
        data, targets = get_batch(train_data, i)
        # Starting each batch, we detach the hidden state from how it was previously produced.
        # If we didn't, the model would try backpropagating all the way to start of the dataset.
        hidden = repackage_hidden(hidden)
        model.zero_grad()
        output, hidden = model(data, hidden)
        loss = criterion(output.view(-1, ntokens), targets)
        loss.backward()

        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
        torch.nn.utils.clip_grad_norm_(model.parameters(), args_clip)
        for p in model.parameters():
            p.data.add_(-lr, p.grad.data)

        total_loss += loss.item()

        if batch % args_log_interval == 0 and batch > 0:
            cur_loss = total_loss / args_log_interval
            elapsed = time.time() - start_time
            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '
                    'loss {:5.2f} | ppl {:8.2f}'.format(
                epoch, batch, len(train_data) // args_bptt, lr,
                elapsed * 1000 / args_log_interval, cur_loss, math.exp(cur_loss)))
            total_loss = 0
            start_time = time.time()

"""Then do the actual training"""

# Loop over epochs.
lr = args_learning_rate
best_val_loss = None

# At any point you can hit Ctrl + C to break out of training early.
try:
    for epoch in range(1, args_num_epochs+1):
        epoch_start_time = time.time()
        train()
        val_loss = evaluate(val_data)
        print('-' * 89)
        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),
                                           val_loss, math.exp(val_loss)))
        print('-' * 89)
        #Save the model if the validation loss is the best we've seen so far.
        if not best_val_loss or val_loss < best_val_loss:
            # with open(args_save, 'wb') as f:
            #     torch.save(model, f)
            bestmodel = copy.deepcopy(model)
            best_val_loss = val_loss
        else:
            # Anneal the learning rate if no improvement has been seen in the validation dataset.
            lr /= 2.0
except KeyboardInterrupt:
    print('-' * 89)
    print('Exiting from training early')

"""Finally,  open the best saved model and run it on the test data"""

# Load the best saved model.
# with open(args_save, 'rb') as f:
#     model = torch.load(f)
model = bestmodel

# Run on test data.
test_loss = evaluate(test_data)
print('=' * 89)
print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(
    test_loss, math.exp(test_loss)))
print('=' * 89)

"""# Word generator

First define the arguments and load the corpus
"""

# Load the best saved model.

# with open(args_save, 'rb') as f:
#     model = torch.load(f)

model = bestmodel

model.eval()

corpus = Corpus(args_data)
ntokens = len(corpus.dictionary)
hidden = model.init_hidden(1)

"""Then generate some data"""

args_words = 300
args_temperature = 0.9
input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)

words = []
probs = []

with torch.no_grad():  # no tracking history
    for i in range(args_words):
        output, hidden = model(input, hidden)

        word_weights = output.squeeze().div(args_temperature).exp().cpu()
        word_idx = torch.multinomial(word_weights, 1)[0]
        input.fill_(word_idx)
        word = corpus.dictionary.idx2word[word_idx]

        # We replace <unk> and <eos> with * to get a cleaner look, but thats just
        # personal preference
        if(word == "<unk>" or word == "<eos>"):
          word = "*"

        print(word + ('\n' if i % 20 == 19 else ' '),end='')

        # We also create arrays with the generated words and their probability
        # to be used for visualizing them in a tool that we created for this
        # purpose
        words.append(word)
        probs.append(output.squeeze()[word_idx].data.tolist())

"""Print the words and probabilities for use in a [vizualization tool](https://github.com/mikkelbrusen/text-weight-visualizer) we created"""

print(words)
print(probs)
